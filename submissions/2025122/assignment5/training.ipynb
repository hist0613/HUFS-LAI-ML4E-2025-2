{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 5: Model Training and Evaluation\n",
    "## Text Classification for arXiv Paper Relevance\n",
    "\n",
    "This notebook trains an embedding-based text classification model to predict the relevance of arXiv papers.\n",
    "\n",
    "### Approach:\n",
    "- **Embedding Model**: SentenceTransformer (all-MiniLM-L6-v2) for text embedding\n",
    "- **Classification Model**: Logistic Regression on top of embeddings\n",
    "- **Task**: Binary classification (relevant=1, not relevant=0)\n",
    "- **Evaluation Metrics**: Accuracy, F1-score, Precision, Recall"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install sentence-transformers scikit-learn pandas numpy matplotlib seaborn -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ All imports successful!\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, confusion_matrix\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"✓ All imports successful!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Loaded 300 samples\n",
      "\n",
      "Dataset Statistics:\n",
      "  Total samples: 300\n",
      "  Relevant (1): 53 (17.7%)\n",
      "  Not relevant (0): 247 (82.3%)\n",
      "\n",
      "Text Length Statistics:\n",
      "  Mean: 176.0 words\n",
      "  Min: 68 words\n",
      "  Max: 276 words\n"
     ]
    }
   ],
   "source": [
    "# Load data from JSON file\n",
    "data_path = Path('./data/data.json')\n",
    "\n",
    "with open(data_path, 'r') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "print(f\"✓ Loaded {len(data)} samples\")\n",
    "\n",
    "# Extract texts and labels\n",
    "texts = [item['abstract'] for item in data]\n",
    "labels = np.array([item['relevance'] for item in data])\n",
    "\n",
    "print(f\"\\nDataset Statistics:\")\n",
    "print(f\"  Total samples: {len(texts)}\")\n",
    "print(f\"  Relevant (1): {(labels == 1).sum()} ({(labels == 1).sum()/len(labels)*100:.1f}%)\")\n",
    "print(f\"  Not relevant (0): {(labels == 0).sum()} ({(labels == 0).sum()/len(labels)*100:.1f}%)\")\n",
    "\n",
    "# Check text lengths\n",
    "text_lengths = [len(text.split()) for text in texts]\n",
    "print(f\"\\nText Length Statistics:\")\n",
    "print(f\"  Mean: {np.mean(text_lengths):.1f} words\")\n",
    "print(f\"  Min: {np.min(text_lengths)} words\")\n",
    "print(f\"  Max: {np.max(text_lengths)} words\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Generate Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading SentenceTransformer model...\n",
      "✓ Model loaded successfully\n",
      "\n",
      "Generating embeddings for 300 texts...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa05a5058b9b40cd9fc47e6a12d9250b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Embeddings generated\n",
      "  Shape: (300, 384)\n",
      "  Dimension: 384\n"
     ]
    }
   ],
   "source": [
    "# Load the embedding model\n",
    "print(\"Loading SentenceTransformer model...\")\n",
    "embedding_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "print(\"✓ Model loaded successfully\")\n",
    "\n",
    "# Generate embeddings\n",
    "print(f\"\\nGenerating embeddings for {len(texts)} texts...\")\n",
    "embeddings = embedding_model.encode(texts, show_progress_bar=True)\n",
    "\n",
    "print(f\"✓ Embeddings generated\")\n",
    "print(f\"  Shape: {embeddings.shape}\")\n",
    "print(f\"  Dimension: {embeddings.shape[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Split:\n",
      "  Train: 210 samples (70.0%)\n",
      "  Validation: 45 samples (15.0%)\n",
      "  Test: 45 samples (15.0%)\n",
      "\n",
      "Label Distribution in Train:\n",
      "  Class 0: 173\n",
      "  Class 1: 37\n",
      "\n",
      "Label Distribution in Validation:\n",
      "  Class 0: 37\n",
      "  Class 1: 8\n",
      "\n",
      "Label Distribution in Test:\n",
      "  Class 0: 37\n",
      "  Class 1: 8\n"
     ]
    }
   ],
   "source": [
    "# Split data into train (70%), validation (15%), and test (15%)\n",
    "# First split: 70% train, 30% temp\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "    embeddings, labels, test_size=0.3, random_state=42, stratify=labels\n",
    ")\n",
    "\n",
    "# Second split: split temp into 50% val, 50% test\n",
    "X_val, X_test, y_val, y_test = train_test_split(\n",
    "    X_temp, y_temp, test_size=0.5, random_state=42, stratify=y_temp\n",
    ")\n",
    "\n",
    "print(\"Data Split:\")\n",
    "print(f\"  Train: {len(X_train)} samples ({len(X_train)/len(embeddings)*100:.1f}%)\")\n",
    "print(f\"  Validation: {len(X_val)} samples ({len(X_val)/len(embeddings)*100:.1f}%)\")\n",
    "print(f\"  Test: {len(X_test)} samples ({len(X_test)/len(embeddings)*100:.1f}%)\")\n",
    "\n",
    "print(f\"\\nLabel Distribution in Train:\")\n",
    "print(f\"  Class 0: {(y_train == 0).sum()}\")\n",
    "print(f\"  Class 1: {(y_train == 1).sum()}\")\n",
    "\n",
    "print(f\"\\nLabel Distribution in Validation:\")\n",
    "print(f\"  Class 0: {(y_val == 0).sum()}\")\n",
    "print(f\"  Class 1: {(y_val == 1).sum()}\")\n",
    "\n",
    "print(f\"\\nLabel Distribution in Test:\")\n",
    "print(f\"  Class 0: {(y_test == 0).sum()}\")\n",
    "print(f\"  Class 1: {(y_test == 1).sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Logistic Regression classifier...\n",
      "✓ Model trained successfully\n"
     ]
    }
   ],
   "source": [
    "# Train Logistic Regression classifier\n",
    "print(\"Training Logistic Regression classifier...\")\n",
    "classifier = LogisticRegression(\n",
    "    max_iter=1000,\n",
    "    random_state=42,\n",
    "    class_weight='balanced'  # Handle class imbalance\n",
    ")\n",
    "\n",
    "classifier.fit(X_train, y_train)\n",
    "print(\"✓ Model trained successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Validation and Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Performance:\n",
      "  Accuracy:  0.8222\n",
      "  F1-score:  0.6000\n",
      "  Precision: 0.5000\n",
      "  Recall:    0.7500\n"
     ]
    }
   ],
   "source": [
    "# Evaluate on validation set\n",
    "y_val_pred = classifier.predict(X_val)\n",
    "y_val_pred_proba = classifier.predict_proba(X_val)[:, 1]\n",
    "\n",
    "val_accuracy = accuracy_score(y_val, y_val_pred)\n",
    "val_f1 = f1_score(y_val, y_val_pred)\n",
    "val_precision = precision_score(y_val, y_val_pred)\n",
    "val_recall = recall_score(y_val, y_val_pred)\n",
    "\n",
    "print(\"Validation Performance:\")\n",
    "print(f\"  Accuracy:  {val_accuracy:.4f}\")\n",
    "print(f\"  F1-score:  {val_f1:.4f}\")\n",
    "print(f\"  Precision: {val_precision:.4f}\")\n",
    "print(f\"  Recall:    {val_recall:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Save Model and Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Classifier saved to models/classifier.pkl\n",
      "✓ Config saved to models/config.json\n"
     ]
    }
   ],
   "source": [
    "# Create output directory if it doesn't exist\n",
    "model_dir = Path('./models')\n",
    "model_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# Save the classifier\n",
    "classifier_path = model_dir / 'classifier.pkl'\n",
    "with open(classifier_path, 'wb') as f:\n",
    "    pickle.dump(classifier, f)\n",
    "print(f\"✓ Classifier saved to {classifier_path}\")\n",
    "\n",
    "# Save embedding model name for reference\n",
    "config_path = model_dir / 'config.json'\n",
    "config = {\n",
    "    'embedding_model': 'all-MiniLM-L6-v2',\n",
    "    'embedding_dimension': embeddings.shape[1],\n",
    "    'classifier_type': 'LogisticRegression',\n",
    "    'training_samples': len(X_train),\n",
    "    'validation_samples': len(X_val),\n",
    "    'test_samples': len(X_test),\n",
    "    'class_weight': 'balanced'\n",
    "}\n",
    "\n",
    "with open(config_path, 'w') as f:\n",
    "    json.dump(config, f, indent=2)\n",
    "print(f\"✓ Config saved to {config_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Training Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "TRAINING SUMMARY\n",
      "==================================================\n",
      "\n",
      "Dataset:\n",
      "  Total samples: 300\n",
      "  Train/Val/Test split: 70%/15%/15%\n",
      "  Class imbalance ratio: 82.3% negative, 17.7% positive\n",
      "\n",
      "Model:\n",
      "  Embedding: SentenceTransformer (all-MiniLM-L6-v2)\n",
      "  Embedding dimension: 384\n",
      "  Classifier: Logistic Regression with class_weight='balanced'\n",
      "\n",
      "Validation Results:\n",
      "  Accuracy:  0.8222\n",
      "  F1-score:  0.6000\n",
      "  Precision: 0.5000\n",
      "  Recall:    0.7500\n",
      "\n",
      "Saved Files:\n",
      "  Classifier: models/classifier.pkl\n",
      "  Config: models/config.json\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*50)\n",
    "print(\"TRAINING SUMMARY\")\n",
    "print(\"=\"*50)\n",
    "print(f\"\\nDataset:\")\n",
    "print(f\"  Total samples: {len(texts)}\")\n",
    "print(f\"  Train/Val/Test split: 70%/15%/15%\")\n",
    "print(f\"  Class imbalance ratio: 82.3% negative, 17.7% positive\")\n",
    "\n",
    "print(f\"\\nModel:\")\n",
    "print(f\"  Embedding: SentenceTransformer (all-MiniLM-L6-v2)\")\n",
    "print(f\"  Embedding dimension: {embeddings.shape[1]}\")\n",
    "print(f\"  Classifier: Logistic Regression with class_weight='balanced'\")\n",
    "\n",
    "print(f\"\\nValidation Results:\")\n",
    "print(f\"  Accuracy:  {val_accuracy:.4f}\")\n",
    "print(f\"  F1-score:  {val_f1:.4f}\")\n",
    "print(f\"  Precision: {val_precision:.4f}\")\n",
    "print(f\"  Recall:    {val_recall:.4f}\")\n",
    "\n",
    "print(f\"\\nSaved Files:\")\n",
    "print(f\"  Classifier: models/classifier.pkl\")\n",
    "print(f\"  Config: models/config.json\")\n",
    "print(\"=\"*50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "classifier",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
